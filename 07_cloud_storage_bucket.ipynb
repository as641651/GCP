{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud storage\n",
    "\n",
    "> Here the data is stored in **buckets**. \n",
    "\n",
    "> They are **scalable** and are **charged based on the size of contents in bucket**\n",
    "\n",
    "> Commonly used for storing **multimedia** or **storing/accessing data for your web app** or **Serving static websites**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are different storage classes for different usecases\n",
    "\n",
    ">1) **Multi-regional** : If the data in bucket is frequently accessed from around the globe (USD 0.025/GB/month)\n",
    "\n",
    ">2) **Regional** : If the data is accessed frequently only from a particular region (USD 0.02/GB/month)\n",
    "\n",
    ">3) **Nearline**: If data is not frequently accessed. Mostly used for backup data (USD 0.01/GB/month). \n",
    "But has access cost of USD 0.01 per GB and charged for minimum 30 days\n",
    "\n",
    ">4) **coldline**: If data is not frequently accessed but required high latency.(USD 0.007/GB/month) Used when restore time of back up data should be quicker. Access cost is more than that of nearline (USD 0.05/GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a bucket\n",
    "\n",
    "> Done via **GCP console** under **Storage > storage**\n",
    "\n",
    "> The **name of the bucket** should be **globally unique**\n",
    "\n",
    "> Creating in **command line** using **[gsutil](https://cloud.google.com/storage/docs/gsutil)** - tool to access cloud storage\n",
    "```bash\n",
    "gsutil mb -p [PROJECT_NAME] -c [STORAGE_CLASS] -l [BUCKET_LOCATION] gs://[BUCKET_NAME]/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all buckets in a project\n",
    "\n",
    ">```bash\n",
    "gsutil ls\n",
    "(or)\n",
    "gsutil list\n",
    "```\n",
    "> For detailed information\n",
    "```bash\n",
    "gsutil list -L -b gs://[BUCKET_NAME]\n",
    "```\n",
    "\n",
    "### To **list files inside a bucket**\n",
    "\n",
    ">```bash\n",
    "gsutil ls gs://[BUCKET_NAME]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking size of the bucket\n",
    "\n",
    ">This is done via commandline. This command displays the size of all buckets under a project\n",
    "```bash\n",
    "gsutil du -s -h\n",
    "```\n",
    "**Note:** -h displays the size in KB/MB/GB\n",
    "\n",
    ">To display size of single bucket\n",
    "```bash\n",
    "gsutil du -s -h gs://[BUCKET_NAME]/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading files\n",
    "\n",
    "> This can be done through the console or through **gsutil**\n",
    "```bash\n",
    "gsutil cp [FILE_PATH] gs://[BUCKET_NAME]\n",
    "```\n",
    "\n",
    "> For directories,\n",
    "```bash\n",
    "gsutil cp -r [DIR_PATH] gs://[BUCKET_NAME]\n",
    "```\n",
    "\n",
    "> To synchronize local directory with a cloud directory directory\n",
    "```bash\n",
    "gsutil rsync -r [DIR_NAME] gs://[BUCKET_NAME]/[DIR_NAME]\n",
    "```\n",
    "\n",
    "> To copy files between buckets\n",
    "```bash\n",
    "gsutil cp -r -p gs://[BUCKET_NAME_1]/* gs://[BUCKET_NAME_2]\n",
    "```\n",
    "**-p** copies over **permission** of all **files**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hosting a static webpage\n",
    "\n",
    "> **Upload** HTML file(s). By default, **permission** is **not public**\n",
    "\n",
    "> Click the three dots next to file name and **edit permissions**.\n",
    "\n",
    ">> Select **User** for the Entity.\n",
    "\n",
    ">> Enter **allUsers** for the Name.\n",
    "    \n",
    ">> Select **Reader** for the Access.\n",
    "\n",
    "> Through **[gsutil acl](https://cloud.google.com/storage/docs/gsutil/commands/acl)**\n",
    "```bash\n",
    "gsutil acl ch -u AllUsers:R gs://[BUCKET_NAME]/[FILE_NAME]\n",
    "```\n",
    "\n",
    "> Your file will now be **public** and there will be an **icon which will redirect you to the URL**\n",
    "\n",
    ">> The URL is of form:\n",
    "```bash\n",
    "https://storage.googleapis.com/[BUCKER_NAME]/[FILE_NAME]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting lifecycle\n",
    "\n",
    "> The lifecycle configuration specifies that all objects in a bucket that are more than certain period old will be deleted automatically or changed to nearline/coldline storage class\n",
    "\n",
    "> They are useful when we want to **manage log files**\n",
    "\n",
    "> This can be applied only to whole bucket and not to individual files\n",
    "\n",
    "> This can be done easily through **console** or through command line as follows\n",
    "```bash\n",
    "gsutil lifecycle set config.json gs://[BUCKET_NAME]\n",
    "```\n",
    ">>**config.json**\n",
    "```json\n",
    "{\n",
    "  \"rule\":\n",
    "  [\n",
    "    {\n",
    "      \"action\": {\"type\": \"Delete\"},\n",
    "      \"condition\": {\"age\": 365} #Files more than 365 days will be deleted\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "> To **get** lifecycle settings for a bucket\n",
    "```bash\n",
    "gsutil lifecycle get gs://[BUCKET_NAME]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer service\n",
    "\n",
    "> Available under **Storage > Transfer**\n",
    "\n",
    "> Used to transfer from other online repositories like **AWS** or from some URL. They can also be used for transfers with GCP. They have additional functionalities like, **scheduling transfers everyday at certain time**, **Deleting files from source bucket after transfer** etc\n",
    "\n",
    "> Prefer **gsutil** for transfers from **local machine** or **transfers within GCP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Service accounts \n",
    "\n",
    "**Provide separate folders in cloud for every VM instance/ local machines **\n",
    "\n",
    "A service account is a special Google account that belongs to your application or a virtual machine (VM), instead of to an individual end user. Your application uses the service account to call the Google API of a service, so that the users aren't directly involved.\n",
    "\n",
    "For example, a Compute Engine VM may run as a service account, and that account can be given permissions to access the resources it needs. This way the service account is the identity of the service, and the service account's permissions control which resources the service can access.\n",
    "\n",
    "> Go to **IAM&Admin > service accounts** and create a service account for your project\n",
    "\n",
    "> Specify the **access** - Editor/Viewer/Owner/etc\n",
    "\n",
    "> Generate a **json key** for that service account\n",
    "\n",
    "> In your new VM instances, place that **json key** and **autherize gcloud**\n",
    "```bash\n",
    "gcloud auth activate-service-account --key-file [JSON_KEY_FILE]\n",
    "gcloud init\n",
    "```\n",
    "\n",
    "> If the permission was **Viewer**, **gsutil** can only **view files** in the cloud. **Cannot create, start or stop VM instances**. If the permission was **editor**, gsutil can **upload** new files to cloud and set permissions.\n",
    "```bash\n",
    "gsutil cp newfile gs://[BUCKET_NAME]\n",
    "gsutil acl set private gs://[BUCKET_NAME]/new_file\n",
    "```\n",
    "This will make the file accessible only from this VM instance (service account)\n",
    "\n",
    "> Thus **each service accounts (VM instances/local computers) can hold private folders in cloud**\n",
    "\n",
    "**Modifying permissions of service account**\n",
    "\n",
    "Permissions can be **changed** or  more permissions can be **added** under **IAM** overview \n",
    "\n",
    "**To access storage buckets from different project**\n",
    "\n",
    "> VM instance in **project-1**\n",
    "\n",
    "> Storage bucket in **project-2**\n",
    "\n",
    "> Create **service account in project 2** with permission **storage viewer** and generate **json key**\n",
    "\n",
    "> Log on to VM instance in **project-1** and use **gcloud auth** with the key\n",
    "\n",
    "> In the Vm instance **gcloud init** with the cross-project service account and set the project as **project-2**\n",
    "\n",
    "> Now we can access the storage in **project-2** with **gsutils** from the VM instance in **project-1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versioning\n",
    "\n",
    "> Enable versioning for cloud bucket\n",
    "\n",
    ">```bash\n",
    "gsutil versioning set [on|off] gs://[BUCKET_NAME]\n",
    "```\n",
    ">  Now we can get a **version number for each file** in the bucket using the command. Everytime we replace the file in the cloud, all the old versions will be available\n",
    "```bash\n",
    "gsutil ls -a gs://[BUCKET_NAME]\n",
    "```\n",
    ">>```bash\n",
    "setup.txt#150987263\n",
    "```\n",
    "\n",
    ">```bash\n",
    "gsutil cp setup.txt gs://[BUCKET_NAME] (Upload another file)\n",
    "gsutil ls -a gs://[BUCKET_NAME]\n",
    "```\n",
    ">>```bash\n",
    "setup.txt#150987263 (Old version)\n",
    "setup.txt#151098234\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
