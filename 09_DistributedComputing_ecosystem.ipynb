{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed computing ecosystem\n",
    "\n",
    "**Data for computation no longer fits in RAM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDFS\n",
    "\n",
    "> When data is **huge**, it is **stored in distributed system**. (**Data warehouse**)\n",
    "\n",
    "> **HDFS** is a paradigm that divides the data into **fixed sized** blocks and stores them in cluster. There is a **master node** which holds **meta data** about where each block is stored. The **equivalent of HDFS** is the **Google cloud storage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map reduce\n",
    "\n",
    "> **Distrubutes computation** across nodes in cluster in two steps :\n",
    "\n",
    ">> **Map :** Maps the data to different nodes. Output of each **node** is a **key-value pair**\n",
    "\n",
    ">> **Reduce**: Carries out some **reduce** opeartion **on the output of each node**. **Reduce** all node outputs with **same key**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YARN\n",
    "\n",
    "> **Yet Another Resource Negotiator** is the heart of **co-ordination operations in clusters**\n",
    "\n",
    "> **Coordinates and scales** tasks running on **cluster**. Allows choosing **scheduling algorithms.**\n",
    "\n",
    "> **Schedules** tasks across nodes, **Assigns new nodes** in case of failure.\n",
    "\n",
    "> This is abstracted as **Spark** on **hadoop clusters.** This is **equivalent** to **Dataproc** on **GCP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop\n",
    "\n",
    "> **Hadoop** is a **java framework** that **abstracts Map reduce**. It consist of three components:\n",
    "\n",
    ">> **HDFS** : File system to manage storage of data\n",
    "\n",
    ">> **MapReduce**: Framework to **process data across multiple servers**\n",
    "\n",
    ">> **YARN** : Framework to **run and manage** data processing tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Query & Hive\n",
    "\n",
    "> **Hadoop** requires knowledge of **java** for **implementation**. This was difficult for many data scientists who had less programming background. Therefore **Big query and Hive** were developed.\n",
    "\n",
    "> **Big query and Hive** are **SQL like interfaces** on top of **map reduce implementation**. Therefore, the users can work as if they as working with traditional RDBMS \n",
    "\n",
    ">> **Hive** works with **HDFS** and has **high latency** (records are not indexed and hence higer access time)\n",
    "\n",
    ">> **Big Query** works with **google cloud storage** and has **lower latency** and is suited for **real-time apps** (Uses **columnar database** which allows indexing) \n",
    "\n",
    "\n",
    "> ### OLAP(Hive) vs OLTP (RDBMS)\n",
    "\n",
    "> We **dont use** Hive or Big Query for **OLTP**(Online transaction processing) use cases (unlike CloudSQL / Cloud spanner). This is because they are more **optimized for read and analytic operations**. Hence Hive and BIgQuery are most suited for **OLAP**(Online Analytic processing) and **business intelligence apps**\n",
    "\n",
    ">> **OLAP** are **not ACID** compliant and any data can be dumped into database.\n",
    "\n",
    ">> **OLTP** are **ACID** compliant. That is, only data which satisfies certain constraints are stored in database\n",
    "\n",
    "> Do **not use** Hive for **finding needle in a haystack applications**. Because, even to fetch a single row, it might take a minute and all the overheads of hadoop are there. **Cloud SQL** is best suited for this case. Use **Hive/BQ** to **analyse or manipulate and store enite large sets of data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Pig (GCP term: Cloud Dataflow)\n",
    "\n",
    "> A high level scripting language used to **clean unstructured and incomplete** data and load them in a **clean HDFS** format in **parallel**.\n",
    "\n",
    ">> Eg, extract data from experiment logs in **parallel**.\n",
    "\n",
    "> It is **available by default** on every **GCP dataproc cluster**\n",
    "\n",
    "> Raw data ---(**pig : ETL **)---> Data warehouse ---(**hive/spark : Analyse**)---> Out\n",
    "\n",
    ">> **ETL** stands for Extract, Transform and Load operation\n",
    "\n",
    "> **Pig** is used by **developers** to **bring together** useful data in **one place**, while **hive** is used by **analysts** to **retrieve** business **information** from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hadoop, Hive and Pig are all managed via Dataproc in GCP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An **general purpose engine** for **data processing and analysis**, which goes beyond the hadoop ecosystem\n",
    "\n",
    "> Used for **applyling machine learning algorithms** on **big data**\n",
    "\n",
    "> Spark is an alternative to hadoop. Spark uses **Resilient distributed database**.\n",
    "\n",
    "> **Spark** provides an interactive shell in a **distriuted programming world**. This is, it has an **interactive shell** over the **map reduce paradigm**\n",
    "\n",
    ">> **PySpark** : Spark binaries for pytyhon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Spark core** (This is a compute engine) runs on top of\n",
    "\n",
    ">> 1) **Storage system** (**HDFS**)\n",
    "\n",
    ">> 2) **Cluster manager** to help spark run tasks across cluster of machines (**YARN**)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When the dataset is **unbounded** (ie, data are continously received from sensors / webcam), they are processed as **streams**\n",
    "\n",
    "> Unbounded datasets require **continous processing (stream processing)**. This is opposed to **batch processing** where the data size is known prior to processing.\n",
    "\n",
    ">> Eg, Continous monitoring and updating of tweets, climate info, logs\n",
    "\n",
    "> **Map reduce cannot be used** for stream processing. It is a **pure batch processingystem**\n",
    "\n",
    "> **Stream-first architecture** consist of\n",
    "\n",
    ">> **Message transport system** : \n",
    "\n",
    ">>> Queue of buffers of streamed data\n",
    "\n",
    ">>> **Kafka and MapR** are the technologies used.\n",
    "\n",
    ">> **Stream processsing system** : \n",
    "\n",
    ">>> **Low latency** and **low fault tolerence overhead** is critical\n",
    "\n",
    ">>> **Order of arrival of data** is important. Any out of order data should be tracked and treated as a special case.\n",
    "\n",
    ">>> **Apache Flink**, **Apache storm** and **Spark Streaming** are the technologies used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
